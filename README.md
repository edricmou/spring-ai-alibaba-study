## 提示词 (Prompt)

Prompt 是引导 AI 模型生成特定输出的输入格式，Prompt 的设计和措辞会显著影响模型的响应。

Prompt 最开始只是简单的字符串，随着时间的推移，prompt 逐渐开始包含特定的占位符，例如 AI 模型可以识别的 “USER:”、“SYSTEM:” 等。阿里云通义模型可通过将多个消息字符串分类为不同的角色，然后再由 AI 模型处理，为 prompt 引入了更多结构。每条消息都分配有特定的角色，这些角色对消息进行分类，明确AI 模型提示的每个部分的上下文和目的。这种结构化方法增强了与 AI 沟通的细微差别和有效性，因为 prompt 的每个部分在交互中都扮演着独特且明确的角色。

提示词的四大角色

- 系统角色（System Role）：定义AI的功能边界和角色。指导 AI 的行为和响应方式，设置 AI 如何解释和回复输入的参数或规则。这类似于在发起对话之前向 AI 提供说明。
- 用户角色（User Role）：代表用户的输入 - 他们向 AI 提出的问题、命令或陈述。这个角色至关重要，因为它构成了 AI 响应的基础。
- 助手角色（Assistant Role）：AI 对用户输入的响应。这不仅仅是一个答案或反应，它对于保持对话的流畅性至关重要。通过跟踪 AI 之前的响应（其“助手角色”消息），系统可确保连贯且上下文相关的交互。助手消息也可能包含功能工具调用请求信息。它就像 AI 中的一个特殊功能，在需要执行特定功能（例如计算、获取数据或不仅仅是说话）时使用。
- 工具/功能角色（Tool/Function Role）：工具/功能角色专注于响应工具调用助手消息返回附加信息。

## 向量数据库

### 什么是向量

Vector 是向量或矢量的意思，向量是数学里的概念，而矢量是物理里的概念，但二者描述的是同一件事。

定义：向量是用于表示具有大小和方向的量。

向量可以在不同的维度空间中定义，最常见的是二维和三维空间中的向量，但理论上也可以有更高维的向量。例如，在二维平面上的一个向量可以写作 (x,y)，这里 x 和 y 分别表示该向量沿两个坐标轴方向上的分量；而在三维空间里，则会有一个额外的 z 坐标，即 (x,y,z)。

### 文本向量化

嵌入(Embedding)的工作原理是将文本、图像和视频转换为称为向量(Vectors)的浮点数数组。这些向量旨在捕捉文本、图像和视频的含义。嵌入数组的长度称为向量的维度(Dimensionality)。

嵌入模型(EmbeddingModel)是嵌入过程中采用的模型。当前EmbeddingModel的接口主要用于将文本转换为数值向量,接口的设计主要围绕这两个目标展开:
- 可移植性:该接口确保在各种嵌入模型之间的轻松适配。它允许开发者在不同的嵌入技术或模型之间切换,所需的代码更改最小化。这一设计与Spring模块化和互换性的理念一致。
- 简单性:嵌入模型简化了文本转换为嵌入的过程。通过提供如embed(String text)和embed(Document document)这样简单的方法,它去除了处理原始文本数据和嵌入算法的复杂性。这个设计选择使开发者,尤其是那些初次接触AI的开发者,更容易在他们的应用程序中使用嵌入,而无需深入了解其底层机制。

### 向量数据库

向量存储（VectorStore）是一种用于存储和检索高维向量数据的数据库或存储解决方案，它特别适用于处理那些经过嵌入模型转化后的数据。在VectorStore中，查询与传统关系数据库不同。它们执行相似性搜索，而不是精确匹配。当给定一个向量作为查询时，VectorStore返回与查询向量“相似”的向量。
VectorStore用于将您的数据与AI模型集成。在使用它们时的第一步是将您的数据加载到矢量数据库中。然后，当要将用户查询发送到AI模型时，首先检索一组相似文档。然后，这些文档作为用户问题的上下文，并与用户的查询一起发送到AI模型。这种技术被称为检索增强生成（Retrieval Augmented Generation，RAG）。
VectorStore API提供了简单易用的接口供开发者对VectorStore进行操作，接下来的部分描述相关接口以及一些高层次的示例用法。

 其核心功能是通过高效的索引结构和相似性计算算法，支持大规模向量数据的快速查询与分析，向量数据库维度越高，查询精准度也越高，查询效果也越好。

## LLM的缺陷

- LLM的知识不是实时的，不具备知识更新.
- LLM可能不知道你私有的领域/业务知识.
- LLM有时会在回答中生成看似合理但实际上是错误的信息

## Rag(Retrieval Augment Generation) 检索增强生成

简单来说，RAG（检索增强生成）是一种从你的数据中查找相关信息，并在将提示发送给LLM之前将其注入到提示中的方法。这样一来，LLM就能获得（希望是）相关的信息，并基于这些信息进行回答，从而降低产生幻觉的概率。  

通过引入外部知识源来增强LLM的输出能力，传统的LLM通常基于其训练数据生成响应，但这些数据可能过时或不够全面。RAG允许模型在生成答案之前，从特定的知识库中检索相关信息，从而提供更准确和上下文相关的回答

Rag使用流程

1. 建立索引。提取各种格式的数据，如PDF, HTML, WORD和Markdown文件，然后将其统一转化为纯文本格式。然后将这些使用使用嵌入模型转化为向量导入到向量数据库，作为知识库。方便后续检索。
2. 检索。在用户提问时， 会使用嵌入模型将用户的提问转化为向量，先在知识库中进行相似性检索，将检索出来的相关数据注入到LLM请求的提示词中，一并发送给大模型，保证LLM回答的准确性和专业性。

### RAG切片方式

如果知识库数据量太大，可以对知识库进行切片存储，有以下6种切片类型

| 切分方法     | 核心适用场景                                                 | 关键要求 / 特点                                              |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 智能切分     | 通用文档，追求在多数文档上实现最佳检索效果                   | 无需人工预设规则，通过算法自动识别语义边界，适配性广         |
| 按长度切分   | 对 Token 数量有严格限制的场景，如使用上下文长度较小的模型时  | 以固定 Token 数或字符数为标准分割，不考虑内容语义连贯性      |
| 按页切分     | 每页内容传达独立主题的文档（如多页报告、PPT）                | 确保不同页面的内容不混杂在同一个文本切片中，严格遵循页面物理边界 |
| 按标题切分   | 用标题（含多级标题）划分并承载独立主题的文档（如论文、说明书） | 以标题层级为分割依据，保证不同标题下的内容不跨切片，结构与文档逻辑一致 |
| 按照正则切分 | 需要自定义复杂分割规则的场景，需精准匹配特定文本格式         | 依赖人工设置的正则表达式，仅对符合表达式规则的文本片段进行分割，灵活性高 |
| 按照符号切分 | 内容通过特定标识符区分的文档                                 | 以文档中的特殊符号（如。、*、#、@等）为分割点，规则简单直接，易操作 |

## Tool Caling(工具调用)

“工具调用（Tool Calling）”或“函数调用”允许大型语言模型（LLM）在必要时调用一个或多个可用的工具，这些工具通常由开发者定义。工具可以是任何东西：网页搜索、对外部 API 的调用，或特定代码的执行等。LLM 本身不能实际调用工具；相反，它们会在响应中表达调用特定工具的意图（而不是以纯文本回应）。然后，应用程序应该执行这个工具，并报告工具执行的结果给模型。当 LLM 可以访问工具时，它可以在合适的情况下决定调用其中一个工具，这是一个非常强大的功能。

## Modal Context Protocol(模型上下文协议)

模型上下文协议（即 Model Context Protocol，MCP）是一个开放协议，它规范了应用程序如何向大型语言模型（LLM）提供上下文。MCP 提供了一种统一的方式将 AI 模型连接到不同的数据源和工具，它定义了统一的集成方式。在开发智能体（Agent）的过程中，我们经常需要将智能体与数据和工具集成，MCP 以标准的方式规范了智能体与数据及工具的集成方式，可以帮助您在 LLM 之上构建智能体（Agent）和复杂的工作流。目前已经有大量的服务接入并提供了 MCP server 实现，当前这个生态正在以非常快的速度不断的丰富中，具体可参见：MCP Servers。

Java界的SpringCloud Openfeign，只不过Openfeign是用于微服务通讯的，而MCP用于大模型通讯的，但它们都是为了通讯获取某项数据的一种机制。

提供了一种标准化的方式来连接LLMs需要的上下文，MCP就类似于一个Agent时代的Type-C协议，希望能将不同来源的数据、工具、服务统一起来供大模型调用。

<img src="./assets/image-20251020231453971.png" alt="image-20251020231453971" style="zoom:25%;" />

- MCP 主机（MCP Hosts）：发起请求的 AI 应用程序，比如聊天机器人、AI 驱动的 IDE 等。
- MCP 客户端（MCP Clients）：在主机程序内部，与 MCP 服务器保持 1:1 的连接。
- MCP 服务器（MCP Servers）：为 MCP 客户端提供上下文、工具和提示信息。
- 本地资源（Local Resources）：本地计算机中可供 MCP 服务器安全访问的资源，如文件、数据库。
- 远程资源（Remote Resources）：MCP 服务器可以连接到的远程资源，如通过 API 提供的数据

## MCP通信协议的两种模式

1. STDIO(标准输入/输出)：支持标准输入和输出流进行通信，主要用于本地集成、命令行工具等场景
2. SSE (Server-Sent Events)：支持使用 HTTP POST请求进行服务器到客户端流式处理，以实现客户端到服务器的通信

| 特性     | SSE                              | STDIO                                |
| -------- | -------------------------------- | ------------------------------------ |
| 传输协议 | HTTP（长连接）                   | 操作系统级文件描述符                 |
| 方向     | 服务器→客户端（单向推送）        | 双向流（stdin，stdout）              |
| 保持连接 | 长连接（Connection: keep-alive） | 不保证长时间打开，取决于进程生命周期 |
| 数据格式 | 文本流（EventStream 格式）       | 原始字节流                           |
| 异常处理 | 可通过 HTTP 状态码或重连机制     | 进程退出或管道断裂                   |

<img src="./assets/image-20251020232136770.png" alt="image-20251020232136770" style="zoom: 33%;" />

<img src="./assets/image-20251021155545953.png" alt="image-20251021155545953" style="zoom:33%;" />